## About The Project
This is my first proper project since learning basic python along with the starter libraries like Pandas, NumPy, Matplotlib. I was inspired to train my own neural network from scratch on the popular MNIST dataset having done the machine learning course by Andrew Ng on Coursera. 

I learned and wanted to implement numerous concepts such as implementing forward propagation and ReLU and Softmax activation functions, calculating cost function, calculating backpropagation and even taking things a step further using the Adam optimiser and mini-batch gradient descent to improve the neural network.

## Dataset
The project utilises the MNIST dataset from the [Kaggle Digit Recogniser](https://www.kaggle.com/competitions/digit-recognizer/data) competition.

![Samples of handwritten digits from the MNIST dataset on Kaggle](https://github.com/user-attachments/assets/4c9f528a-8345-426d-a9a9-2fdc99713a11)

## Project Structure
The project consists of just [one jupyter notebook](https://github.com/ethancqy/portfolio/blob/main/Neural%20Network%20from%20Scratch%20(MNIST)/Neural%20Network%20from%20Scratch%20(MNIST).ipynb) detailing my learning and implementation of the neural network, along with the results, do take a look!

```
Neural Network from Scratch (MNIST)/
├── Neural Network from Scratch (MNIST).ipynb
├── README.md
├── fig_2.1.1.png
├── fig_2.2.1.png
├── fig_4.2.1.png
└── fig_4.2.2.webp
```

